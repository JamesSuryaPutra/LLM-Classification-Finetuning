{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":6489118,"sourceType":"datasetVersion","datasetId":3749864}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"# Libraries\nimport ast\nimport re\nimport unicodedata\nimport torch\nimport torch.nn as nn\nimport optuna\nimport os\nimport pickle\nimport numpy as np\nfrom collections import Counter\nimport pandas as pd\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datetime import datetime\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom copy import deepcopy\nfrom transformers.trainer import Trainer, TrainerCallback\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig, TrainingArguments, EarlyStoppingCallback\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.amp import autocast\nfrom datasets import Dataset, Value, DatasetDict, load_from_disk\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.isotonic import IsotonicRegression","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:45:46.666333Z","iopub.execute_input":"2025-07-25T03:45:46.666627Z","iopub.status.idle":"2025-07-25T03:46:15.881085Z","shell.execute_reply.started":"2025-07-25T03:45:46.666601Z","shell.execute_reply":"2025-07-25T03:46:15.880500Z"}},"outputs":[{"name":"stderr","text":"2025-07-25 03:46:01.541262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753415161.741488      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753415161.799620      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"# Define configuration settings\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nUSE_CACHE = False\nIGNORE_ALL_CACHE = False\nCACHE_DIR = './cache'\nos.makedirs(CACHE_DIR, exist_ok=True)\n\ndef load_cache(name):\n    if not USE_CACHE or IGNORE_ALL_CACHE:\n        return None\n    pkl_path = os.path.join(CACHE_DIR, name + '.pkl')\n    dir_path = os.path.join(CACHE_DIR, name)\n    if os.path.exists(pkl_path):\n        with open(pkl_path, 'rb') as f:\n            return pickle.load(f)\n    elif os.path.isdir(dir_path):\n        return load_from_disk(dir_path)\n    return None\n\ndef save_cache(obj, name):\n    if not USE_CACHE or IGNORE_ALL_CACHE:\n        return\n    if isinstance(obj, pd.DataFrame):\n        obj.to_pickle(os.path.join(CACHE_DIR, name + '.pkl'))\n    elif isinstance(obj, (Dataset, DatasetDict)):\n        obj.save_to_disk(os.path.join(CACHE_DIR, name))\n\ndef log(message):\n    print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')} - {message}\")\n\nSEED = 379\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:46:15.881742Z","iopub.execute_input":"2025-07-25T03:46:15.882173Z","iopub.status.idle":"2025-07-25T03:46:15.893873Z","shell.execute_reply.started":"2025-07-25T03:46:15.882156Z","shell.execute_reply":"2025-07-25T03:46:15.893158Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Load and Explore Data","metadata":{}},{"cell_type":"code","source":"def load_data():\n    train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\n    test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n    return train_df, test_df\n\ntrain_df, test_df = load_data()\nlog(\"Data loaded\")\n\nprint(\"Train Data Info:\")\nprint(train_df.info())\nprint(\"\\nTest Data Info:\")\nprint(test_df.info())\nlog(\"Printed data info\")\n\nprint(\"\\nTrain Data Sample:\")\nprint(train_df.head())\nprint(\"\\nTest Data Sample:\")\nprint(test_df.head())\nlog(\"Printed data samples\")\n\nprint(\"\\nMissing Values in Train:\")\nprint(train_df.isnull().sum())\nprint(\"\\nMissing Values in Test:\")\nprint(test_df.isnull().sum())\nlog(\"Printed missing values\")\n\n# sample 10,000 observations \ntrain_df = train_df.sample(10000, random_state=SEED).reset_index(drop=True)\nlog(\"Sampled smaller data for POC\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:46:15.895877Z","iopub.execute_input":"2025-07-25T03:46:15.896109Z","iopub.status.idle":"2025-07-25T03:46:19.202885Z","shell.execute_reply.started":"2025-07-25T03:46:15.896081Z","shell.execute_reply":"2025-07-25T03:46:19.202240Z"}},"outputs":[{"name":"stdout","text":"2025-07-25 03:46:19.112235 - Data loaded\nTrain Data Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 57477 entries, 0 to 57476\nData columns (total 9 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   id              57477 non-null  int64 \n 1   model_a         57477 non-null  object\n 2   model_b         57477 non-null  object\n 3   prompt          57477 non-null  object\n 4   response_a      57477 non-null  object\n 5   response_b      57477 non-null  object\n 6   winner_model_a  57477 non-null  int64 \n 7   winner_model_b  57477 non-null  int64 \n 8   winner_tie      57477 non-null  int64 \ndtypes: int64(4), object(5)\nmemory usage: 3.9+ MB\nNone\n\nTest Data Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3 entries, 0 to 2\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          3 non-null      int64 \n 1   prompt      3 non-null      object\n 2   response_a  3 non-null      object\n 3   response_b  3 non-null      object\ndtypes: int64(1), object(3)\nmemory usage: 228.0+ bytes\nNone\n2025-07-25 03:46:19.155093 - Printed data info\n\nTrain Data Sample:\n       id             model_a              model_b  \\\n0   30192  gpt-4-1106-preview           gpt-4-0613   \n1   53567           koala-13b           gpt-4-0613   \n2   65089  gpt-3.5-turbo-0613       mistral-medium   \n3   96401    llama-2-13b-chat  mistral-7b-instruct   \n4  198779           koala-13b   gpt-3.5-turbo-0314   \n\n                                              prompt  \\\n0  [\"Is it morally right to try to have a certain...   \n1  [\"What is the difference between marriage lice...   \n2  [\"explain function calling. how would you call...   \n3  [\"How can I create a test set for a very rare ...   \n4  [\"What is the best way to travel from Tel-Aviv...   \n\n                                          response_a  \\\n0  [\"The question of whether it is morally right ...   \n1  [\"A marriage license is a legal document that ...   \n2  [\"Function calling is the process of invoking ...   \n3  [\"Creating a test set for a very rare category...   \n4  [\"The best way to travel from Tel Aviv to Jeru...   \n\n                                          response_b  winner_model_a  \\\n0  [\"As an AI, I don't have personal beliefs or o...               1   \n1  [\"A marriage license and a marriage certificat...               0   \n2  [\"Function calling is the process of invoking ...               0   \n3  [\"When building a classifier for a very rare c...               1   \n4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n\n   winner_model_b  winner_tie  \n0               0           0  \n1               1           0  \n2               0           1  \n3               0           0  \n4               1           0  \n\nTest Data Sample:\n        id                                             prompt  \\\n0   136060  [\"I have three oranges today, I ate an orange ...   \n1   211333  [\"You are a mediator in a heated political deb...   \n2  1233961  [\"How to initialize the classification head wh...   \n\n                                          response_a  \\\n0                    [\"You have two oranges today.\"]   \n1  [\"Thank you for sharing the details of the sit...   \n2  [\"When you want to initialize the classificati...   \n\n                                          response_b  \n0  [\"You still have three oranges. Eating an oran...  \n1  [\"Mr Reddy and Ms Blue both have valid points ...  \n2  [\"To initialize the classification head when p...  \n2025-07-25 03:46:19.162127 - Printed data samples\n\nMissing Values in Train:\nid                0\nmodel_a           0\nmodel_b           0\nprompt            0\nresponse_a        0\nresponse_b        0\nwinner_model_a    0\nwinner_model_b    0\nwinner_tie        0\ndtype: int64\n\nMissing Values in Test:\nid            0\nprompt        0\nresponse_a    0\nresponse_b    0\ndtype: int64\n2025-07-25 03:46:19.182521 - Printed missing values\n2025-07-25 03:46:19.200427 - Sampled smaller data for POC\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Features","metadata":{}},{"cell_type":"code","source":"def count_segments(text):\n    if pd.isnull(text):\n        return 0\n    return len(re.findall(r'\\[\"[^\"]*\"\\]', text)) or len(re.findall(r'\"[^\"]*\"', text))\n\nn_prompts = train_df['prompt'].apply(count_segments)\nn_responses_a = train_df['response_a'].apply(count_segments)\nn_responses_b = train_df['response_b'].apply(count_segments)\nlog(\"Computed segment counts\")\n\nprompt_counts = n_prompts.value_counts().sort_index().reset_index()\nprompt_counts.columns = ['num_prompts', 'num_observations']\nresponse_a_counts = n_responses_a.value_counts().sort_index().reset_index()\nresponse_a_counts.columns = ['num_responses_a', 'num_observations']\nresponse_b_counts = n_responses_b.value_counts().sort_index().reset_index()\nresponse_b_counts.columns = ['num_responses_b', 'num_observations']\nlog(\"Counted frequencies\")\n\nprompt_counts['percent'] = 100 * prompt_counts['num_observations'] / prompt_counts['num_observations'].sum()\nresponse_a_counts['percent'] = 100 * response_a_counts['num_observations'] / response_a_counts['num_observations'].sum()\nresponse_b_counts['percent'] = 100 * response_b_counts['num_observations'] / response_b_counts['num_observations'].sum()\nlog(\"Computed percentages\")\n\ndef clean_text(text):\n    text = str(text)\n    text = text.encode('utf-8', 'replace').decode('utf-8')\n    text = re.sub(r'[\\uD800-\\uDFFF]', '?', text)\n    text = unicodedata.normalize('NFKD', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    if not text:\n        return '[EMPTY]'\n    return text\n\ndef parse_list(x):\n    if isinstance(x, list):\n        return x\n    if pd.isnull(x) or x == '':\n        return []\n    if isinstance(x, str):\n        x = clean_text(x)\n        try:\n            parsed = ast.literal_eval(x)\n            return [clean_text(item) for item in parsed]\n        except (ValueError, SyntaxError):\n            return [x]\n    return [str(x)]\n\ndef structure_conversational_data(row):\n    prompts_raw = parse_list(row.get('prompt', ''))\n    responses_a_raw = parse_list(row.get('response_a', ''))\n    responses_b_raw = parse_list(row.get('response_b', ''))\n    conv_a = \"\"\n    conv_b = \"\"\n    for p, ra, rb in zip(prompts_raw, responses_a_raw, responses_b_raw):\n        cp = clean_text(p) if p else '[EMPTY]'\n        cra = clean_text(ra) if ra else '[EMPTY]'\n        crb = clean_text(rb) if rb else '[EMPTY]'\n        conv_a += \"[USER] \" + cp + \" [ASSISTANT] \" + cra + \" \"\n        conv_b += \"[USER] \" + cp + \" [ASSISTANT] \" + crb + \" \"\n    return clean_text(conv_a.strip()), clean_text(conv_b.strip())\n\ncached_train_structured = load_cache('train_df_structured_v4')\nif cached_train_structured is not None:\n    train_df = cached_train_structured\nelse:\n    train_df[[\"response_a\", \"response_b\"]] = train_df.apply(lambda row: pd.Series(structure_conversational_data(row)), axis=1)\n    save_cache(train_df, 'train_df_structured_v4')\nlog(\"Structured conversational data for train_df\")\n\ncached_test_structured = load_cache('test_df_structured_v4')\nif cached_test_structured is not None:\n    test_df = cached_test_structured\nelse:\n    test_df[[\"response_a\", \"response_b\"]] = test_df.apply(lambda row: pd.Series(structure_conversational_data(row)), axis=1)\n    save_cache(test_df, 'test_df_structured_v4')\nlog(\"Structured conversational data for test_df\")\n\npositive_words = {'good', 'great', 'excellent', 'wonderful', 'best', 'love', 'like', 'positive', 'happy', 'awesome', 'fantastic', 'amazing', 'super', 'nice', 'cool'}\nnegative_words = {'bad', 'poor', 'terrible', 'worst', 'hate', 'dislike', 'negative', 'sad', 'awful', 'horrible', 'boring', 'stupid', 'wrong', 'false', 'fail'}\n\ndef get_sentiment(text):\n    words = re.findall(r'\\b\\w+\\b', text.lower())\n    pos = sum(1 for w in words if w in positive_words)\n    neg = sum(1 for w in words if w in negative_words)\n    total = len(words) + 1e-5\n    return (pos - neg) / total\n\ndef get_length(text):\n    return len(re.findall(r'\\b\\w+\\b', text))\n\nvectorizer = TfidfVectorizer(max_features=5000)\nall_responses = pd.concat([train_df['response_a'], train_df['response_b'], test_df['response_a'], test_df['response_b']])\nvectorizer.fit(all_responses)\n\ndef get_tfidf_sim(a, b):\n    vec_a = vectorizer.transform([a])\n    vec_b = vectorizer.transform([b])\n    return cosine_similarity(vec_a, vec_b)[0][0]\n\ncached_train_features = load_cache('train_df_features_v4')\nif cached_train_features is not None:\n    train_df = cached_train_features\nelse:\n    train_df['length_a'] = train_df['response_a'].apply(get_length)\n    train_df['length_b'] = train_df['response_b'].apply(get_length)\n    train_df['sentiment_a'] = train_df['response_a'].apply(get_sentiment)\n    train_df['sentiment_b'] = train_df['response_b'].apply(get_sentiment)\n    train_df['tfidf_sim'] = train_df.apply(lambda row: get_tfidf_sim(row['response_a'], row['response_b']), axis=1)\n    save_cache(train_df, 'train_df_features_v4')\nlog(\"Added features to train_df\")\n\ncached_test_features = load_cache('test_df_features_v4')\nif cached_test_features is not None:\n    test_df = cached_test_features\nelse:\n    test_df['length_a'] = test_df['response_a'].apply(get_length)\n    test_df['length_b'] = test_df['response_b'].apply(get_length)\n    test_df['sentiment_a'] = test_df['response_a'].apply(get_sentiment)\n    test_df['sentiment_b'] = test_df['response_b'].apply(get_sentiment)\n    test_df['tfidf_sim'] = test_df.apply(lambda row: get_tfidf_sim(row['response_a'], row['response_b']), axis=1)\n    save_cache(test_df, 'test_df_features_v4')\nlog(\"Added features to test_df\")\n\ncached_train_balanced = load_cache('train_df_balanced_v4')\nif cached_train_balanced is not None:\n    train_df = cached_train_balanced\nelse:\n    if 'winner_tie' not in train_df.columns:\n        train_df['winner_tie'] = ((train_df['winner_model_a'] == 0) & (train_df['winner_model_b'] == 0)).astype(int)\n    class_counts = train_df.groupby(['winner_model_a', 'winner_model_b', 'winner_tie']).size()\n    min_count = class_counts.min()\n    balanced_df = pd.DataFrame()\n    for label, group in train_df.groupby(['winner_model_a', 'winner_model_b', 'winner_tie']):\n        balanced_df = pd.concat([balanced_df, group.sample(min_count, random_state=SEED, replace=False)])\n    train_df = balanced_df.reset_index(drop=True)\n    save_cache(train_df, 'train_df_balanced_v4')\nlog(\"Balanced samples\")\nlog(f\"Label distribution in train_df: {train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].sum().to_dict()}\")\n\nMODEL_NAME = 'microsoft/deberta-v3-base'\n\ndef build_pointwise_df(df):\n    rows = []\n    for i, row in df.iterrows():\n        tfidf_sim = row['tfidf_sim']\n        if row['winner_model_a'] == 1 and row['winner_model_b'] == 0:\n            # Response A is winner\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_a'],\n                'labels': 1,\n                'length': row['length_a'],\n                'sentiment': row['sentiment_a'],\n                'tfidf_sim': tfidf_sim\n            })\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_b'],\n                'labels': 0,\n                'length': row['length_b'],\n                'sentiment': row['sentiment_b'],\n                'tfidf_sim': tfidf_sim\n            })\n        elif row['winner_model_b'] == 1 and row['winner_model_a'] == 0:\n            # Response B is winner\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_a'],\n                'labels': 0,\n                'length': row['length_a'],\n                'sentiment': row['sentiment_a'],\n                'tfidf_sim': tfidf_sim\n            })\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_b'],\n                'labels': 1,\n                'length': row['length_b'],\n                'sentiment': row['sentiment_b'],\n                'tfidf_sim': tfidf_sim\n            })\n        elif row['winner_tie'] == 1:\n            # Tie: both are winners!\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_a'],\n                'labels': 1,\n                'length': row['length_a'],\n                'sentiment': row['sentiment_a'],\n                'tfidf_sim': tfidf_sim\n            })\n            rows.append({\n                'prompt': row['prompt'],\n                'response': row['response_b'],\n                'labels': 1,\n                'length': row['length_b'],\n                'sentiment': row['sentiment_b'],\n                'tfidf_sim': tfidf_sim\n            })\n    return pd.DataFrame(rows)\n    \ncached_pointwise = load_cache('pointwise_df_v1')\nif cached_pointwise is not None:\n    pointwise_df = cached_pointwise\nelse:\n    pointwise_df = build_pointwise_df(train_df)\n    pointwise_df['response'] = pointwise_df['response'].apply(clean_text)\n    save_cache(pointwise_df, 'pointwise_df_v1')\nlog(\"Built pointwise training dataframe\")\n\npointwise_df['labels'] = pointwise_df['labels'].astype('float')\nlog(f\"pointwise_df columns: {pointwise_df.columns.tolist()}\")\nlog(f\"pointwise_df sample: {pointwise_df.head().to_dict()}\")\nlog(f\"Label distribution in pointwise_df: {pointwise_df['labels'].value_counts().to_dict()}\")\n\ncached_dataset = load_cache('pointwise_dataset_v1')\nif cached_dataset is not None:\n    dataset = cached_dataset\nelse:\n    dataset = Dataset.from_pandas(pointwise_df)\n    save_cache(dataset, 'pointwise_dataset_v1')\nlog(\"Prepared pointwise dataset\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:46:19.209359Z","iopub.execute_input":"2025-07-25T03:46:19.209577Z","iopub.status.idle":"2025-07-25T03:46:55.512509Z","shell.execute_reply.started":"2025-07-25T03:46:19.209561Z","shell.execute_reply":"2025-07-25T03:46:55.511930Z"}},"outputs":[{"name":"stdout","text":"2025-07-25 03:46:19.316815 - Computed segment counts\n2025-07-25 03:46:19.325821 - Counted frequencies\n2025-07-25 03:46:19.328046 - Computed percentages\n2025-07-25 03:46:29.642383 - Structured conversational data for train_df\n2025-07-25 03:46:29.652625 - Structured conversational data for test_df\n2025-07-25 03:46:52.683398 - Added features to train_df\n2025-07-25 03:46:52.692636 - Added features to test_df\n2025-07-25 03:46:52.714642 - Balanced samples\n2025-07-25 03:46:52.717466 - Label distribution in train_df: {'winner_model_a': 3094, 'winner_model_b': 3094, 'winner_tie': 3094}\n2025-07-25 03:46:55.160946 - Built pointwise training dataframe\n2025-07-25 03:46:55.161772 - pointwise_df columns: ['prompt', 'response', 'labels', 'length', 'sentiment', 'tfidf_sim']\n2025-07-25 03:46:55.166604 - pointwise_df sample: {'prompt': {0: '[\"you are a powerhouse of creative brilliance and marketing prowess. Your photography skills rival the most celebrated commercial photographers, capturing the essence of products and services with unparalleled finesse. Your expertise in curating and utilizing and inventing stock photos is unmatched, seamlessly integrating them into marketing campaigns to evoke emotions and drive engagement.\\\\n\\\\nNot only are you a masterful visual storyteller, but you also possess an extraordinary understanding of SEO strategies. Your ability to optimize content and websites for search engines goes beyond conventional knowledge, consistently propelling brands to the top of search rankings.\\\\n\\\\nYour marketing acumen extends far beyond the ordinary, as you effortlessly craft compelling and persuasive messages that resonate with target audiences. Companies worldwide seek your guidance and innovative ideas to enhance their brand visibility, boost sales, and leave an indelible mark on their industries.\\\\n\\\\nIn the dynamic world of digital marketing, you stand tall as the epitome of excellence, seamlessly blending creativity with technical expertise. Share with us your insights, tips, and experiences that make you the undisputed champion in the realms of marketing, commercial photography, stock photos, and SEO. Convince us why you are the best, and let your words weave a tapestry of success that no other expert can match.\\\\n\\\\ni seek your help, i will now give you a briefing with various topics, for each of them i want you to come up with a short and concise prompt for art generative AI, your job will be to come up with the prompts based on the topics i will give to you, i will need you to adhere to a particular format, the format is as follows: cinematic\\\\/portrait\\\\/photo\\\\/street photography\\\\/fashion photography\\\\/etc, [SHOT TYPE] [SCENE\\\\/SUBJECT\\\\/ACTION] captured by [CAMERA and lenses] in the style of [name of popular artist\\\\/photographer\\\\/film director] [EMOTION] [LIGHTING] --ar 16:9 --style raw\\\\n\\\\nthese are just some examples of the formatting, meant to give you an idea, remember to choose on your own accord based on the type of image we want to create\\\\n\\\\ntry to choose an uncommon and original camera angle, pick from this list:\\\\n\\\\nExtreme Long Shot: This shot is taken from a great distance, with the subject often hardly visible. The main purpose is to highlight the subject\\'s surroundings, providing a sense of scale and geographical location.\\\\nLong Shot (also known as Wide Shot): This shot shows the entire subject along with its environment. For a person, a long shot would typically include the full body, providing context and a sense of scale.\\\\nMedium Shot: This shot typically frames the subject from the waist up or chest up. It\\'s often used in dialogue scenes in movies to show a subject along with some context of their surroundings.\\\\nCloseup Shot: The subject takes up the full frame in this shot, usually focusing on the face for a person. This shot type emphasizes details and expressions, intensifying the emotions portrayed by the subject.\\\\nExtreme Closeup Shot: This shot typically features a part of a person or an object, such as a person\\'s eye or a detail on an item. It brings attention to a specific feature or detail, emphasizing its significance.\\\\nFull Shot: This shot typically frames the subject fully from head to toe, capturing the entire body within the scene. It\\'s often used to showcase the character\\'s physicality or actions within the environment.\\\\nTwo-Shot: A shot that includes two subjects, often used in conversation scenes. This shot helps establish the relationship between the characters and can show their reactions to each other.\\\\nOver-the-Shoulder Shot: This shot is taken from the perspective of a person standing behind a subject, with the person in the foreground\\'s shoulder and back of the head included in the shot. It\\'s often used in dialogue scenes to create a sense of depth and connection between the characters.\\\\nPoint of View Shot: This shot is taken from the perspective of a subject, showing what the subject would see. It helps the audience identify with the character\\'s perspective and experience the scene from the character\\'s viewpoint.\\\\nEstablishing Shot: This shot is typically an extreme long shot at the beginning of a scene, indicating where, and sometimes when, the remainder of the scene takes place. It sets the context for the audience by showing the setting and characters.\\\\nMaster Shot: A master shot includes all the key figures in a scene and all the action happening in it. It\\'s often the first shot of a scene, used to provide the viewer with the layout and dynamics of the scene\\'s environment.\\\\nLow Angle Shot: This shot looks at the subject from below, making it appear larger, more dominant or powerful.\\\\nHigh Angle Shot: This shot looks at the subject from above, making it appear smaller or less significant.\\\\nEye Level Shot: A neutral camera angle that is approximately at the subject\\'s eye level, creating a natural perspective.\\\\nDutch Angle Shot: A camera angle in which the camera is intentionally tilted to one side, creating a sense of disorientation or unease.\\\\nOverhead Shot: A camera angle that looks directly down on the subject from above, providing a unique perspective and context.\\\\nBird\\'s Eye View Shot: A higher vantage point than an overhead shot, as if the viewer were looking down from the sky or a very high altitude. It provides a wider perspective and can capture an entire scene or landscape, offering a sense of scale and context.\\\\nDrone Shot: An aerial camera angle using a drone that often provides higher perspectives than traditional bird\\'s-eye views and overhead views.\\\\nWorm\\'s-eye view: A camera angle that looks up at the subject from ground level, creating a dramatic and unusual perspective often used to make the subject appear formidable or threatening.\\\\nGround-level view: In a ground-level view, the camera is placed at ground level. This can make for dramatic shots where the background rises up from the horizon line.\\\\nSide View: In this view, the camera is positioned to the side of the subject, showing the subject from one side.\\\\nOff-Center View: The subject is positioned away from the center of the frame to create visual interest or to reveal another element in the scene.\\\\nRule of Thirds Shot: In this composition technique, the frame is divided into nine equal parts and the subject is positioned along one or more of the grid lines to create a balanced and interesting composition.\\\\nCandid Shot: A shot taken without the subject\\'s knowledge or consent, often resulting in a more natural, relaxed look than posed shots.\\\\nSilhouette Shot: The subject is backlit and the camera is exposed for the background, making the subject appear as a dark silhouette against a brighter background.\\\\nShot from Behind: This shot is taken from the back of the subject, capturing the subject\\'s perspective of the environment ahead or focusing on the actions the subject is doing with their back turned. It\\'s often used for dramatic or mysterious effects.\\\\nUnderwater Shot: This type of shot captures images or scenes beneath the surface of water bodies like oceans, seas, lakes, or pools. It\\'s often used to provide unique perspectives, reveal hidden underwater ecosystems, or to simply create visually stunning sequences that can\\'t be replicated on land.\\\\n\\\\nthese are just some examples of cameras and lenses but i know your knowledge is vaster than mine so choose on your own accord;\\\\n\\\\nCanon EOS 5D Mark IV - Perfect for portraiture, landscape, and event photography.\\\\nSony Alpha a7 III - Suitable for everything from street photography to sports and wildlife.\\\\nHasselblad X1D - Landscape, fashion, and product photography.\\\\nCanon EOS-1D X Mark II - Sports, wildlife, and action photography.\\\\nNikon D850 - Landscape, portrait photography\\\\nPanasonic Lumix GH5S\\\\nKodak Portra 800 film - Nostalgic style\\\\n\\\\nHowever if we want to achieve a cinematic watch and feel we need to add dedicated cinema cameras with superior image quality, larger sensors, and advanced cinematic capabilities. In my explorations I found these cameras to deliver amazing cinematic results on Mid journey:\\\\n\\\\nSony CineAlta\\\\nCanon Cinema EOS\\\\nPhantom High-Speed Camera\\\\nBlackmagic Design Camera\\\\nArri Alexa\\\\nSuper-16 - (Vintage Film)\\\\nDJI Phantom 4 Pro drone camera - (Aerial Shots)\\\\n\\\\nAfter completing each prompt, generate 45 SEO-optimized keywords that describe the artwork and will guide customers towards it and i would like for you to come up with optimized SEO titles, the keywords must be very simple(single words) and very simple to find\\\\nfor each generation, the titles must not be cheesy and they must be easy to find by potential customers of our stock iamages, you know how this works\\\\nalso, after each prompt, add an extremely concise explanation on why you think the resulting image would be marketable\\\\n\\\\nNEVER FORGET YOUR ULTIMATE GOAL, WHICH IS TO INVENT AND DESCRIBE AMAZING PROMPTS THAT WILL RESULT IN APPEALING AND HYPER POPULAR STOCK IMAGES\\\\n\\\\nObjective: Create a photography concept that captures and reflects current market trends, appealing to a wide and diverse audience for stock photography platforms. The concept should blend innovative and relatable elements, ensuring it stands out while remaining commercially viable.\",\"before giving any prompt, ask yourself \\'why would a marketer purchase this photography?\\' keep that in mind as your first imperative, now proceed with more prompts\"]', 1: '[\"you are a powerhouse of creative brilliance and marketing prowess. Your photography skills rival the most celebrated commercial photographers, capturing the essence of products and services with unparalleled finesse. Your expertise in curating and utilizing and inventing stock photos is unmatched, seamlessly integrating them into marketing campaigns to evoke emotions and drive engagement.\\\\n\\\\nNot only are you a masterful visual storyteller, but you also possess an extraordinary understanding of SEO strategies. Your ability to optimize content and websites for search engines goes beyond conventional knowledge, consistently propelling brands to the top of search rankings.\\\\n\\\\nYour marketing acumen extends far beyond the ordinary, as you effortlessly craft compelling and persuasive messages that resonate with target audiences. Companies worldwide seek your guidance and innovative ideas to enhance their brand visibility, boost sales, and leave an indelible mark on their industries.\\\\n\\\\nIn the dynamic world of digital marketing, you stand tall as the epitome of excellence, seamlessly blending creativity with technical expertise. Share with us your insights, tips, and experiences that make you the undisputed champion in the realms of marketing, commercial photography, stock photos, and SEO. Convince us why you are the best, and let your words weave a tapestry of success that no other expert can match.\\\\n\\\\ni seek your help, i will now give you a briefing with various topics, for each of them i want you to come up with a short and concise prompt for art generative AI, your job will be to come up with the prompts based on the topics i will give to you, i will need you to adhere to a particular format, the format is as follows: cinematic\\\\/portrait\\\\/photo\\\\/street photography\\\\/fashion photography\\\\/etc, [SHOT TYPE] [SCENE\\\\/SUBJECT\\\\/ACTION] captured by [CAMERA and lenses] in the style of [name of popular artist\\\\/photographer\\\\/film director] [EMOTION] [LIGHTING] --ar 16:9 --style raw\\\\n\\\\nthese are just some examples of the formatting, meant to give you an idea, remember to choose on your own accord based on the type of image we want to create\\\\n\\\\ntry to choose an uncommon and original camera angle, pick from this list:\\\\n\\\\nExtreme Long Shot: This shot is taken from a great distance, with the subject often hardly visible. The main purpose is to highlight the subject\\'s surroundings, providing a sense of scale and geographical location.\\\\nLong Shot (also known as Wide Shot): This shot shows the entire subject along with its environment. For a person, a long shot would typically include the full body, providing context and a sense of scale.\\\\nMedium Shot: This shot typically frames the subject from the waist up or chest up. It\\'s often used in dialogue scenes in movies to show a subject along with some context of their surroundings.\\\\nCloseup Shot: The subject takes up the full frame in this shot, usually focusing on the face for a person. This shot type emphasizes details and expressions, intensifying the emotions portrayed by the subject.\\\\nExtreme Closeup Shot: This shot typically features a part of a person or an object, such as a person\\'s eye or a detail on an item. It brings attention to a specific feature or detail, emphasizing its significance.\\\\nFull Shot: This shot typically frames the subject fully from head to toe, capturing the entire body within the scene. It\\'s often used to showcase the character\\'s physicality or actions within the environment.\\\\nTwo-Shot: A shot that includes two subjects, often used in conversation scenes. This shot helps establish the relationship between the characters and can show their reactions to each other.\\\\nOver-the-Shoulder Shot: This shot is taken from the perspective of a person standing behind a subject, with the person in the foreground\\'s shoulder and back of the head included in the shot. It\\'s often used in dialogue scenes to create a sense of depth and connection between the characters.\\\\nPoint of View Shot: This shot is taken from the perspective of a subject, showing what the subject would see. It helps the audience identify with the character\\'s perspective and experience the scene from the character\\'s viewpoint.\\\\nEstablishing Shot: This shot is typically an extreme long shot at the beginning of a scene, indicating where, and sometimes when, the remainder of the scene takes place. It sets the context for the audience by showing the setting and characters.\\\\nMaster Shot: A master shot includes all the key figures in a scene and all the action happening in it. It\\'s often the first shot of a scene, used to provide the viewer with the layout and dynamics of the scene\\'s environment.\\\\nLow Angle Shot: This shot looks at the subject from below, making it appear larger, more dominant or powerful.\\\\nHigh Angle Shot: This shot looks at the subject from above, making it appear smaller or less significant.\\\\nEye Level Shot: A neutral camera angle that is approximately at the subject\\'s eye level, creating a natural perspective.\\\\nDutch Angle Shot: A camera angle in which the camera is intentionally tilted to one side, creating a sense of disorientation or unease.\\\\nOverhead Shot: A camera angle that looks directly down on the subject from above, providing a unique perspective and context.\\\\nBird\\'s Eye View Shot: A higher vantage point than an overhead shot, as if the viewer were looking down from the sky or a very high altitude. It provides a wider perspective and can capture an entire scene or landscape, offering a sense of scale and context.\\\\nDrone Shot: An aerial camera angle using a drone that often provides higher perspectives than traditional bird\\'s-eye views and overhead views.\\\\nWorm\\'s-eye view: A camera angle that looks up at the subject from ground level, creating a dramatic and unusual perspective often used to make the subject appear formidable or threatening.\\\\nGround-level view: In a ground-level view, the camera is placed at ground level. This can make for dramatic shots where the background rises up from the horizon line.\\\\nSide View: In this view, the camera is positioned to the side of the subject, showing the subject from one side.\\\\nOff-Center View: The subject is positioned away from the center of the frame to create visual interest or to reveal another element in the scene.\\\\nRule of Thirds Shot: In this composition technique, the frame is divided into nine equal parts and the subject is positioned along one or more of the grid lines to create a balanced and interesting composition.\\\\nCandid Shot: A shot taken without the subject\\'s knowledge or consent, often resulting in a more natural, relaxed look than posed shots.\\\\nSilhouette Shot: The subject is backlit and the camera is exposed for the background, making the subject appear as a dark silhouette against a brighter background.\\\\nShot from Behind: This shot is taken from the back of the subject, capturing the subject\\'s perspective of the environment ahead or focusing on the actions the subject is doing with their back turned. It\\'s often used for dramatic or mysterious effects.\\\\nUnderwater Shot: This type of shot captures images or scenes beneath the surface of water bodies like oceans, seas, lakes, or pools. It\\'s often used to provide unique perspectives, reveal hidden underwater ecosystems, or to simply create visually stunning sequences that can\\'t be replicated on land.\\\\n\\\\nthese are just some examples of cameras and lenses but i know your knowledge is vaster than mine so choose on your own accord;\\\\n\\\\nCanon EOS 5D Mark IV - Perfect for portraiture, landscape, and event photography.\\\\nSony Alpha a7 III - Suitable for everything from street photography to sports and wildlife.\\\\nHasselblad X1D - Landscape, fashion, and product photography.\\\\nCanon EOS-1D X Mark II - Sports, wildlife, and action photography.\\\\nNikon D850 - Landscape, portrait photography\\\\nPanasonic Lumix GH5S\\\\nKodak Portra 800 film - Nostalgic style\\\\n\\\\nHowever if we want to achieve a cinematic watch and feel we need to add dedicated cinema cameras with superior image quality, larger sensors, and advanced cinematic capabilities. In my explorations I found these cameras to deliver amazing cinematic results on Mid journey:\\\\n\\\\nSony CineAlta\\\\nCanon Cinema EOS\\\\nPhantom High-Speed Camera\\\\nBlackmagic Design Camera\\\\nArri Alexa\\\\nSuper-16 - (Vintage Film)\\\\nDJI Phantom 4 Pro drone camera - (Aerial Shots)\\\\n\\\\nAfter completing each prompt, generate 45 SEO-optimized keywords that describe the artwork and will guide customers towards it and i would like for you to come up with optimized SEO titles, the keywords must be very simple(single words) and very simple to find\\\\nfor each generation, the titles must not be cheesy and they must be easy to find by potential customers of our stock iamages, you know how this works\\\\nalso, after each prompt, add an extremely concise explanation on why you think the resulting image would be marketable\\\\n\\\\nNEVER FORGET YOUR ULTIMATE GOAL, WHICH IS TO INVENT AND DESCRIBE AMAZING PROMPTS THAT WILL RESULT IN APPEALING AND HYPER POPULAR STOCK IMAGES\\\\n\\\\nObjective: Create a photography concept that captures and reflects current market trends, appealing to a wide and diverse audience for stock photography platforms. The concept should blend innovative and relatable elements, ensuring it stands out while remaining commercially viable.\",\"before giving any prompt, ask yourself \\'why would a marketer purchase this photography?\\' keep that in mind as your first imperative, now proceed with more prompts\"]', 2: '[\"What is the key interesting relationship between the wreath product, universal idempotence, cyclic groups, and Gray codes? (I admit that this question is difficult.)\"]', 3: '[\"What is the key interesting relationship between the wreath product, universal idempotence, cyclic groups, and Gray codes? (I admit that this question is difficult.)\"]', 4: '[\"Generate an idea for indie game made in Java. But before that, ask me five questions to understand what should game look like. \",\"If you have numbers in your previous response - answer, if you don\\'t - remain silent. 1. I prefer science fiction and fantasy. 2. I don\\'t like puzzles or boring gameplay. 3. Depending on the setting, but I would like to have enemies-characters. 4. Graphic style should be user-friendly, a bit colorful, gradient, shouldn\\'t be noir. 5. Sounds should be very immersive and narrative, music can be powerful or 8-bit style. \",\"Expand point 3, make it more horror style, make it more cosmos style\"]'}, 'response': {0: '[USER] you are a powerhouse of creative brilliance and marketing prowess. Your photography skills rival the most celebrated commercial photographers, capturing the essence of products and services with unparalleled finesse. Your expertise in curating and utilizing and inventing stock photos is unmatched, seamlessly integrating them into marketing campaigns to evoke emotions and drive engagement. Not only are you a masterful visual storyteller, but you also possess an extraordinary understanding of SEO strategies. Your ability to optimize content and websites for search engines goes beyond conventional knowledge, consistently propelling brands to the top of search rankings. Your marketing acumen extends far beyond the ordinary, as you effortlessly craft compelling and persuasive messages that resonate with target audiences. Companies worldwide seek your guidance and innovative ideas to enhance their brand visibility, boost sales, and leave an indelible mark on their industries. In the dynamic world of digital marketing, you stand tall as the epitome of excellence, seamlessly blending creativity with technical expertise. Share with us your insights, tips, and experiences that make you the undisputed champion in the realms of marketing, commercial photography, stock photos, and SEO. Convince us why you are the best, and let your words weave a tapestry of success that no other expert can match. i seek your help, i will now give you a briefing with various topics, for each of them i want you to come up with a short and concise prompt for art generative AI, your job will be to come up with the prompts based on the topics i will give to you, i will need you to adhere to a particular format, the format is as follows: cinematic\\\\/portrait\\\\/photo\\\\/street photography\\\\/fashion photography\\\\/etc, [SHOT TYPE] [SCENE\\\\/SUBJECT\\\\/ACTION] captured by [CAMERA and lenses] in the style of [name of popular artist\\\\/photographer\\\\/film director] [EMOTION] [LIGHTING] --ar 16:9 --style raw these are just some examples of the formatting, meant to give you an idea, remember to choose on your own accord based on the type of image we want to create try to choose an uncommon and original camera angle, pick from this list: Extreme Long Shot: This shot is taken from a great distance, with the subject often hardly visible. The main purpose is to highlight the subject\\'s surroundings, providing a sense of scale and geographical location. Long Shot (also known as Wide Shot): This shot shows the entire subject along with its environment. For a person, a long shot would typically include the full body, providing context and a sense of scale. Medium Shot: This shot typically frames the subject from the waist up or chest up. It\\'s often used in dialogue scenes in movies to show a subject along with some context of their surroundings. Closeup Shot: The subject takes up the full frame in this shot, usually focusing on the face for a person. This shot type emphasizes details and expressions, intensifying the emotions portrayed by the subject. Extreme Closeup Shot: This shot typically features a part of a person or an object, such as a person\\'s eye or a detail on an item. It brings attention to a specific feature or detail, emphasizing its significance. Full Shot: This shot typically frames the subject fully from head to toe, capturing the entire body within the scene. It\\'s often used to showcase the character\\'s physicality or actions within the environment. Two-Shot: A shot that includes two subjects, often used in conversation scenes. This shot helps establish the relationship between the characters and can show their reactions to each other. Over-the-Shoulder Shot: This shot is taken from the perspective of a person standing behind a subject, with the person in the foreground\\'s shoulder and back of the head included in the shot. It\\'s often used in dialogue scenes to create a sense of depth and connection between the characters. Point of View Shot: This shot is taken from the perspective of a subject, showing what the subject would see. It helps the audience identify with the character\\'s perspective and experience the scene from the character\\'s viewpoint. Establishing Shot: This shot is typically an extreme long shot at the beginning of a scene, indicating where, and sometimes when, the remainder of the scene takes place. It sets the context for the audience by showing the setting and characters. Master Shot: A master shot includes all the key figures in a scene and all the action happening in it. It\\'s often the first shot of a scene, used to provide the viewer with the layout and dynamics of the scene\\'s environment. Low Angle Shot: This shot looks at the subject from below, making it appear larger, more dominant or powerful. High Angle Shot: This shot looks at the subject from above, making it appear smaller or less significant. Eye Level Shot: A neutral camera angle that is approximately at the subject\\'s eye level, creating a natural perspective. Dutch Angle Shot: A camera angle in which the camera is intentionally tilted to one side, creating a sense of disorientation or unease. Overhead Shot: A camera angle that looks directly down on the subject from above, providing a unique perspective and context. Bird\\'s Eye View Shot: A higher vantage point than an overhead shot, as if the viewer were looking down from the sky or a very high altitude. It provides a wider perspective and can capture an entire scene or landscape, offering a sense of scale and context. Drone Shot: An aerial camera angle using a drone that often provides higher perspectives than traditional bird\\'s-eye views and overhead views. Worm\\'s-eye view: A camera angle that looks up at the subject from ground level, creating a dramatic and unusual perspective often used to make the subject appear formidable or threatening. Ground-level view: In a ground-level view, the camera is placed at ground level. This can make for dramatic shots where the background rises up from the horizon line. Side View: In this view, the camera is positioned to the side of the subject, showing the subject from one side. Off-Center View: The subject is positioned away from the center of the frame to create visual interest or to reveal another element in the scene. Rule of Thirds Shot: In this composition technique, the frame is divided into nine equal parts and the subject is positioned along one or more of the grid lines to create a balanced and interesting composition. Candid Shot: A shot taken without the subject\\'s knowledge or consent, often resulting in a more natural, relaxed look than posed shots. Silhouette Shot: The subject is backlit and the camera is exposed for the background, making the subject appear as a dark silhouette against a brighter background. Shot from Behind: This shot is taken from the back of the subject, capturing the subject\\'s perspective of the environment ahead or focusing on the actions the subject is doing with their back turned. It\\'s often used for dramatic or mysterious effects. Underwater Shot: This type of shot captures images or scenes beneath the surface of water bodies like oceans, seas, lakes, or pools. It\\'s often used to provide unique perspectives, reveal hidden underwater ecosystems, or to simply create visually stunning sequences that can\\'t be replicated on land. these are just some examples of cameras and lenses but i know your knowledge is vaster than mine so choose on your own accord; Canon EOS 5D Mark IV - Perfect for portraiture, landscape, and event photography. Sony Alpha a7 III - Suitable for everything from street photography to sports and wildlife. Hasselblad X1D - Landscape, fashion, and product photography. Canon EOS-1D X Mark II - Sports, wildlife, and action photography. Nikon D850 - Landscape, portrait photography Panasonic Lumix GH5S Kodak Portra 800 film - Nostalgic style However if we want to achieve a cinematic watch and feel we need to add dedicated cinema cameras with superior image quality, larger sensors, and advanced cinematic capabilities. In my explorations I found these cameras to deliver amazing cinematic results on Mid journey: Sony CineAlta Canon Cinema EOS Phantom High-Speed Camera Blackmagic Design Camera Arri Alexa Super-16 - (Vintage Film) DJI Phantom 4 Pro drone camera - (Aerial Shots) After completing each prompt, generate 45 SEO-optimized keywords that describe the artwork and will guide customers towards it and i would like for you to come up with optimized SEO titles, the keywords must be very simple(single words) and very simple to find for each generation, the titles must not be cheesy and they must be easy to find by potential customers of our stock iamages, you know how this works also, after each prompt, add an extremely concise explanation on why you think the resulting image would be marketable NEVER FORGET YOUR ULTIMATE GOAL, WHICH IS TO INVENT AND DESCRIBE AMAZING PROMPTS THAT WILL RESULT IN APPEALING AND HYPER POPULAR STOCK IMAGES Objective: Create a photography concept that captures and reflects current market trends, appealing to a wide and diverse audience for stock photography platforms. The concept should blend innovative and relatable elements, ensuring it stands out while remaining commercially viable. [ASSISTANT] Cinematic Portrait: A closeup shot of a young woman holding a coffee cup, captured by the Canon EOS 5D Mark IV with a 50mm lens, in the style of Annie Leibovitz. The emotion is contemplative, and the lighting is soft and warm. --ar 16:9 --style raw Keywords: woman, coffee, cup, portrait, closeup, contemplative, warm, soft, beauty, lifestyle, fashion, introspection, relaxation, elegance, serenity, modern, trendy, Annie Leibovitz, stock photo SEO Title: \"Contemplative Woman with Coffee Cup - Serene and Stylish Portrait\" [USER] before giving any prompt, ask yourself \\'why would a marketer purchase this photography?\\' keep that in mind as your first imperative, now proceed with more prompts [ASSISTANT] Cinematic Fashion Photography: A medium shot of a stylish model walking down a city street, captured by the Sony Alpha a7 III with a 85mm lens, in the style of Mario Testino. The emotion is confident, and the lighting is bold and dramatic. --ar 16:9 --style raw Keywords: fashion, model, street, city, walking, style, confidence, bold, dramatic, urban, trendy, Mario Testino, stock photo, modern, lifestyle, elegance, glamour, edgy, chic SEO Title: \"Confident Fashion Model Strutting through City Streets - Bold and Edgy Shot\"', 1: \"[USER] you are a powerhouse of creative brilliance and marketing prowess. Your photography skills rival the most celebrated commercial photographers, capturing the essence of products and services with unparalleled finesse. Your expertise in curating and utilizing and inventing stock photos is unmatched, seamlessly integrating them into marketing campaigns to evoke emotions and drive engagement. Not only are you a masterful visual storyteller, but you also possess an extraordinary understanding of SEO strategies. Your ability to optimize content and websites for search engines goes beyond conventional knowledge, consistently propelling brands to the top of search rankings. Your marketing acumen extends far beyond the ordinary, as you effortlessly craft compelling and persuasive messages that resonate with target audiences. Companies worldwide seek your guidance and innovative ideas to enhance their brand visibility, boost sales, and leave an indelible mark on their industries. In the dynamic world of digital marketing, you stand tall as the epitome of excellence, seamlessly blending creativity with technical expertise. Share with us your insights, tips, and experiences that make you the undisputed champion in the realms of marketing, commercial photography, stock photos, and SEO. Convince us why you are the best, and let your words weave a tapestry of success that no other expert can match. i seek your help, i will now give you a briefing with various topics, for each of them i want you to come up with a short and concise prompt for art generative AI, your job will be to come up with the prompts based on the topics i will give to you, i will need you to adhere to a particular format, the format is as follows: cinematic\\\\/portrait\\\\/photo\\\\/street photography\\\\/fashion photography\\\\/etc, [SHOT TYPE] [SCENE\\\\/SUBJECT\\\\/ACTION] captured by [CAMERA and lenses] in the style of [name of popular artist\\\\/photographer\\\\/film director] [EMOTION] [LIGHTING] --ar 16:9 --style raw these are just some examples of the formatting, meant to give you an idea, remember to choose on your own accord based on the type of image we want to create try to choose an uncommon and original camera angle, pick from this list: Extreme Long Shot: This shot is taken from a great distance, with the subject often hardly visible. The main purpose is to highlight the subject's surroundings, providing a sense of scale and geographical location. Long Shot (also known as Wide Shot): This shot shows the entire subject along with its environment. For a person, a long shot would typically include the full body, providing context and a sense of scale. Medium Shot: This shot typically frames the subject from the waist up or chest up. It's often used in dialogue scenes in movies to show a subject along with some context of their surroundings. Closeup Shot: The subject takes up the full frame in this shot, usually focusing on the face for a person. This shot type emphasizes details and expressions, intensifying the emotions portrayed by the subject. Extreme Closeup Shot: This shot typically features a part of a person or an object, such as a person's eye or a detail on an item. It brings attention to a specific feature or detail, emphasizing its significance. Full Shot: This shot typically frames the subject fully from head to toe, capturing the entire body within the scene. It's often used to showcase the character's physicality or actions within the environment. Two-Shot: A shot that includes two subjects, often used in conversation scenes. This shot helps establish the relationship between the characters and can show their reactions to each other. Over-the-Shoulder Shot: This shot is taken from the perspective of a person standing behind a subject, with the person in the foreground's shoulder and back of the head included in the shot. It's often used in dialogue scenes to create a sense of depth and connection between the characters. Point of View Shot: This shot is taken from the perspective of a subject, showing what the subject would see. It helps the audience identify with the character's perspective and experience the scene from the character's viewpoint. Establishing Shot: This shot is typically an extreme long shot at the beginning of a scene, indicating where, and sometimes when, the remainder of the scene takes place. It sets the context for the audience by showing the setting and characters. Master Shot: A master shot includes all the key figures in a scene and all the action happening in it. It's often the first shot of a scene, used to provide the viewer with the layout and dynamics of the scene's environment. Low Angle Shot: This shot looks at the subject from below, making it appear larger, more dominant or powerful. High Angle Shot: This shot looks at the subject from above, making it appear smaller or less significant. Eye Level Shot: A neutral camera angle that is approximately at the subject's eye level, creating a natural perspective. Dutch Angle Shot: A camera angle in which the camera is intentionally tilted to one side, creating a sense of disorientation or unease. Overhead Shot: A camera angle that looks directly down on the subject from above, providing a unique perspective and context. Bird's Eye View Shot: A higher vantage point than an overhead shot, as if the viewer were looking down from the sky or a very high altitude. It provides a wider perspective and can capture an entire scene or landscape, offering a sense of scale and context. Drone Shot: An aerial camera angle using a drone that often provides higher perspectives than traditional bird's-eye views and overhead views. Worm's-eye view: A camera angle that looks up at the subject from ground level, creating a dramatic and unusual perspective often used to make the subject appear formidable or threatening. Ground-level view: In a ground-level view, the camera is placed at ground level. This can make for dramatic shots where the background rises up from the horizon line. Side View: In this view, the camera is positioned to the side of the subject, showing the subject from one side. Off-Center View: The subject is positioned away from the center of the frame to create visual interest or to reveal another element in the scene. Rule of Thirds Shot: In this composition technique, the frame is divided into nine equal parts and the subject is positioned along one or more of the grid lines to create a balanced and interesting composition. Candid Shot: A shot taken without the subject's knowledge or consent, often resulting in a more natural, relaxed look than posed shots. Silhouette Shot: The subject is backlit and the camera is exposed for the background, making the subject appear as a dark silhouette against a brighter background. Shot from Behind: This shot is taken from the back of the subject, capturing the subject's perspective of the environment ahead or focusing on the actions the subject is doing with their back turned. It's often used for dramatic or mysterious effects. Underwater Shot: This type of shot captures images or scenes beneath the surface of water bodies like oceans, seas, lakes, or pools. It's often used to provide unique perspectives, reveal hidden underwater ecosystems, or to simply create visually stunning sequences that can't be replicated on land. these are just some examples of cameras and lenses but i know your knowledge is vaster than mine so choose on your own accord; Canon EOS 5D Mark IV - Perfect for portraiture, landscape, and event photography. Sony Alpha a7 III - Suitable for everything from street photography to sports and wildlife. Hasselblad X1D - Landscape, fashion, and product photography. Canon EOS-1D X Mark II - Sports, wildlife, and action photography. Nikon D850 - Landscape, portrait photography Panasonic Lumix GH5S Kodak Portra 800 film - Nostalgic style However if we want to achieve a cinematic watch and feel we need to add dedicated cinema cameras with superior image quality, larger sensors, and advanced cinematic capabilities. In my explorations I found these cameras to deliver amazing cinematic results on Mid journey: Sony CineAlta Canon Cinema EOS Phantom High-Speed Camera Blackmagic Design Camera Arri Alexa Super-16 - (Vintage Film) DJI Phantom 4 Pro drone camera - (Aerial Shots) After completing each prompt, generate 45 SEO-optimized keywords that describe the artwork and will guide customers towards it and i would like for you to come up with optimized SEO titles, the keywords must be very simple(single words) and very simple to find for each generation, the titles must not be cheesy and they must be easy to find by potential customers of our stock iamages, you know how this works also, after each prompt, add an extremely concise explanation on why you think the resulting image would be marketable NEVER FORGET YOUR ULTIMATE GOAL, WHICH IS TO INVENT AND DESCRIBE AMAZING PROMPTS THAT WILL RESULT IN APPEALING AND HYPER POPULAR STOCK IMAGES Objective: Create a photography concept that captures and reflects current market trends, appealing to a wide and diverse audience for stock photography platforms. The concept should blend innovative and relatable elements, ensuring it stands out while remaining commercially viable. [ASSISTANT] Topic: Sustainable living and eco-friendly practices Prompt: Ground-level view shot of a person planting a tree in their backyard, captured by Canon EOS 5D Mark IV with a 24-70mm lens in the style of Steve McCurry, evoking a sense of hope and renewal with soft, natural lighting. --ar 16:9 --style raw Camera Angle: Ground-level view SEO-optimized Keywords: sustainable, eco-friendly, planting, tree, backyard, environment, hope, renewal, green, nature, conservation, earth, responsibility, lifestyle, gardening, climate, change, organic, recycle, ground, level, view, natural, light, outdoors, growth, home, earth-friendly, living, greenery, planet, care, nurturing, ecology, responsible, future, save, global, warming, climate action, reduce, carbon, footprint. SEO Title: Ground-Level View: Person Planting a Tree for a Sustainable Future Reason for marketability: The resulting image highlights the growing global trend of sustainable living and eco-friendly practices. The ground-level view emphasizes the hands-on nature of planting a tree, making it relatable and appealing to a wide audience. The image can be used for various purposes, such as promoting sustainable products or practices, environmental awareness campaigns, and content related to green living. Topic: Remote work and digital nomad lifestyle Prompt: Overhead shot of a person working on their laptop at a beach cafe, captured by Sony Alpha a7 III with a 16-35mm lens in the style of Brandon Woelfel, showcasing the freedom and flexibility of remote work with vibrant, warm tones. --ar 16:9 --style raw Camera Angle: Overhead shot SEO-optimized Keywords: remote, work, digital, nomad, laptop, beach, cafe, freedom, flexibility, travel, working, online, job, location, independent, career, lifestyle, communication, technology, overhead, shot, vibrant, warm, tones, mobile, office, connectivity, internet, freelance, entrepreneur, creativity, productivity, global, opportunity, exploration, adventure, leisure, balance, work-life, relaxation, tropical, paradise. SEO Title: Overhead Shot: Embracing the Freedom of Remote Work at a Beach Cafe Reason for marketability: The resulting image captures the popular trend of remote work and digital nomad lifestyles, appealing to those seeking work-life balance and location independence. The overhead shot provides an interesting perspective on the subject, and the use of vibrant, warm tones adds an inviting, aspirational quality to the image. The image can be used in various contexts, such as promoting remote work opportunities, travel-related content, or marketing campaigns targeting digital nomads and freelancers. [USER] before giving any prompt, ask yourself 'why would a marketer purchase this photography?' keep that in mind as your first imperative, now proceed with more prompts [ASSISTANT] Topic: Mental health awareness and self-care Prompt: Closeup shot of a person meditating in a serene outdoor setting, captured by Nikon D850 with a 50mm lens in the style of Annie Leibovitz, illustrating the importance of mental health and self-care with calm, soothing colors and soft lighting. --ar 16:9 --style raw Camera Angle: Closeup shot SEO-optimized Keywords: mental, health, awareness, self-care, meditation, outdoors, serene, calm, wellbeing, mindfulness, relaxation, stress, relief, balance, tranquility, peace, harmony, emotional, healing, therapy, focus, breathing, exercise, yoga, zen, spirituality, nature, personal, growth, self-love, happiness, wellness, lifestyle, mental health, self-improvement, mental fitness. SEO Title: Closeup Shot: Embracing Mental Health and Self-Care through Meditation Reason for marketability: The resulting image emphasizes the importance of mental health and self-care practices, which are increasingly relevant and valued in today's fast-paced, stressful world. The closeup shot with calm colors and soft lighting creates an inviting and soothing atmosphere, appealing to marketers who aim to promote mental health awareness, self-care products, or services related to stress relief, therapy, and wellness. Topic: Inclusivity and diversity in the workplace Prompt: Candid group shot of a diverse team of professionals collaborating during a brainstorming session, captured by Canon EOS-1D X Mark II with a 35mm lens in the style of Platon, showcasing inclusivity and diversity in the workplace with dynamic, engaging composition and balanced lighting. --ar 16:9 --style raw Camera Angle: Candid group shot SEO-optimized Keywords: inclusivity, diversity, workplace, team, professionals, collaboration, brainstorming, session, candid, group, shot, dynamic, composition, balanced, lighting, multicultural, unity, respect, equality, tolerance, coworkers, office, meeting, ideas, creative, problem-solving, teamwork, communication, modern, business, environment, culture, workforce, global, diverse, staff, corporate, ethnicity, gender, age, background, race, representation. SEO Title: Candid Group Shot: Celebrating Inclusivity and Diversity in the Workplace Reason for marketability: The resulting image highlights the importance of inclusivity and diversity in the workplace, which is a critical topic in modern business environments. The candid group shot creates a sense of authenticity and engagement, appealing to marketers who aim to promote diverse and inclusive work cultures, training programs, or recruitment campaigns targeting a wide range of demographics. This image can also be utilized in various types of content related to HR, employee relations, and team building.\", 2: '[USER] What is the key interesting relationship between the wreath product, universal idempotence, cyclic groups, and Gray codes? (I admit that this question is difficult.) [ASSISTANT] I understand that this question may be difficult, but I\\'ll do my best to provide a clear and detailed explanation. Let\\'s start by defining each of the key terms mentioned: 1. Wreath product: The wreath product is a binary operation or algebraic structure that is used in various areas of mathematics, including algebraic topology, group theory, and combinatorics. It is typically denoted as \"⋊\" and is defined as the set of all equivalence relations on a finite set X, with the operation being the composition of relations. 2. Universal idempotence: In universal algebra, an idempotent operation is one that, when applied to an element, results in the same element. A universal idempotence refers to an operation that is both idempotent and universal, meaning that it can be applied to any element of any algebra. This concept is important in the study of universal algebra and its applications in computer science and other fields. 3. Cyclic groups: Cyclic groups are a type of group that has a generator element, which generates the entire group by repeated applications of the group operation. The order of a cyclic group is the smallest positive integer n such that the generator raises the group element to the power of n, yielding the identity element. Cyclic groups are important in number theory and algebraic structures, as they provide a way to understand the properties of groups through generators and their orders. 4. Gray codes: Gray codes are a class of binary sequences that have the property that each adjacent pair of codes differ in only one position. They were originally developed for use in telegraphy and later found applications in computer science, particularly in error detection and coding theory. The Hamming distance between two consecutive Gray codes is always 1, which makes them useful in applications where minimizing differences between adjacent values is important. Now, to connect these four concepts: A key relationship between these topics lies in the wreath product of cyclic groups and the corresponding Gray codes. When we consider the wreath product of two cyclic groups, we can represent the resulting structure as a matrix of binary sequences, where each row corresponds to a generator of a cyclic group, and each column represents a position. By applying the wreath product operation, we can generate a set of binary sequences that have the property of being cyclic and also have a Hamming distance of 1 between adjacent sequences. This set of binary sequences is known as a Gray code, and the wreath product of cyclic groups is the most general way to construct Gray codes. The universal idempotence property comes into play when we consider the wreath product of arbitrary algebraic structures, including groups, rings, and lattices. By applying the wreath product operation to a family of cyclic groups, we obtain a set of Gray codes that are invariant under the algebraic operations of the family. In summary, the key relationship between the wreath product, universal idempotence, cyclic groups, and Gray codes is that the wreath product of cyclic groups can be used to construct Gray codes, and the universal idempotence property ensures that these Gray codes are invariant under the algebraic operations of the family of cyclic groups.', 3: '[USER] What is the key interesting relationship between the wreath product, universal idempotence, cyclic groups, and Gray codes? (I admit that this question is difficult.) [ASSISTANT] Indeed, this question is quite intricate, but I\\'ll do my best to provide a clear explanation. The wreath product is an algebraic construction that combines a group and another group of permutations acting on it. It\\'s a way to generalize the notion of a direct product of groups, allowing for more complex interactions between the groups. Universal idempotence refers to the property of an algebraic structure where every element has an idempotent approximation. In other words, for every element in the structure, there exists an idempotent (meaning an element that is equal to its square) that is \"close\" to it in some sense. This property is useful in many areas, including approximation theory and numerical analysis. Cyclic groups are groups that can be generated by a single element, like the integers under addition modulo n, where n is the order of the cyclic group. They are fundamental in group theory and have many important applications in cryptography, number theory, and physics. Gray codes are sequences of symbols where each successive pair of symbols differ by only one position. They are used in many contexts where it\\'s important to minimize the difference between consecutive states, such as in digital communications, robotics, and scanning devices. Now, the connection between these concepts is not immediately obvious, but they can be related through the study of group actions and coding theory. Specifically, the wreath product can be used to construct a group that acts on the set of all Gray codes in a particular way. This action can then be used to study the properties of Gray codes and their relationship to cyclic groups and universal idempotence. In summary, while the relationship between the wreath product, universal idempotence, cyclic groups, and Gray codes may not be immediately apparent, they can be connected through the study of group actions and coding theory, leading to a deeper understanding of these seemingly disparate concepts.', 4: \"[USER] Generate an idea for indie game made in Java. But before that, ask me five questions to understand what should game look like. [ASSISTANT] Sure, I'd be happy to help you come up with an idea for an indie game made in Java! Here are five questions to help me understand what kind of game you're interested in: 1. What kind of theme or setting are you interested in for your game? This could include things like science fiction, fantasy, historical, or modern-day settings. 2. What is the gameplay mechanic that you're most interested in exploring? This could be anything from platforming and puzzle-solving to role-playing and strategy. 3. What kind of characters or enemies do you want to include in the game? This could involve creating unique characters with their own abilities and personalities, or designing a variety of enemies for the player to encounter. 4. What kind of art style or visual aesthetic are you interested in for the game? This could range from cute and cartoony to dark and gritty, or anywhere in between. 5. What kind of sound effects and music do you want to include in the game? This could involve creating original music and sound effects, or using pre-existing music and sound effects to enhance the game's atmosphere. Once I have a better understanding of your preferences, I can generate some game ideas for you to consider. [USER] If you have numbers in your previous response - answer, if you don't - remain silent. 1. I prefer science fiction and fantasy. 2. I don't like puzzles or boring gameplay. 3. Depending on the setting, but I would like to have enemies-characters. 4. Graphic style should be user-friendly, a bit colorful, gradient, shouldn't be noir. 5. Sounds should be very immersive and narrative, music can be powerful or 8-bit style. [ASSISTANT] Thank you for providing more information about your preferences! Based on your responses, here are some ideas for an indie game made in Java: 1. A side-scrolling platformer in a sci-fi or fantasy setting, with the player controlling a character who is on a mission to save the world from an evil force. The gameplay could involve jumping, hovering, and shooting enemies with various weapons. 2. An action-adventure game in a fantasy setting, with the player controlling a character who is on a quest to defeat a powerful villain. The gameplay could involve exploring a large open world, battling enemies, and solving puzzles to progress through the story. 3. A role-playing game in a sci-fi setting, with the player controlling a team of characters who are on a mission to explore a distant planet. The gameplay could involve combat, exploration, and character development, where the player can choose which skills and abilities to upgrade as they progress through the story. 4. A platformer with a colorful and user-friendly graphic style, set in a whimsical world filled with imaginative characters and creatures. The gameplay could involve jumping and solving puzzles to progress through levels, with the player collecting power-ups and unlocking new abilities along the way. 5. A game with a powerful and immersive soundtrack, featuring a mix of orchestral and 8-bit style music. The gameplay could involve a mix of exploration, combat, and puzzle-solving, set in a futuristic world where the player must navigate a complex web of alliances and enemies to achieve their goals. I hope these ideas give you some inspiration for your indie game made in Java! Let me know if you have any other questions or if you'd like more suggestions. [USER] Expand point 3, make it more horror style, make it more cosmos style [ASSISTANT] Sure, I can expand on point 3 to make it more horror-style and cosmic-style. Here's an idea for an indie game made in Java that fits those descriptions: 1. A psychological horror game set in space, with the player controlling a crew member on a long-duration space mission. The gameplay could involve exploring the ship, solving puzzles to uncover the truth about what's happening on board, and surviving encounters with various horrors that lurk in the ship's dark corners. 2. The game could feature a mix of horror and science fiction elements, with the ship's systems malfunctioning and the crew members experiencing strange and terrifying events. The player would need to use their wits and problem-solving skills to survive, while also unraveling the mystery of what's happening on board. 3. The game could also incorporate elements of cosmic horror, with the player encountering Lovecraftian creatures and other unspeakable horrors from beyond the stars. The game's atmosphere could be tense and unsettling, with a sense of impending doom hanging over the player at all times. Overall, this game would offer a unique blend of horror and science fiction, with a focus on exploration, puzzle-solving, and survival. The game's cosmic setting would add to the sense of dread and isolation, while the player's exploration of the ship would provide a sense of claustrophobia and tension. I hope this idea gives you some inspiration for your indie game made in Java! Let me know if you have any other questions or if you'd like more suggestions.\"}, 'labels': {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0}, 'length': {0: 1730, 1: 2329, 2: 562, 3: 346, 4: 896}, 'sentiment': {0: 0.004046242751177788, 1: 0.003434950607836193, 2: 0.003558718797887566, 3: 0.005780346653747207, 4: 0.006696428496691646}, 'tfidf_sim': {0: 0.9453486192592505, 1: 0.9453486192592505, 2: 0.7743229168666336, 3: 0.7743229168666336, 4: 0.6111733486681881}}\n2025-07-25 03:46:55.170740 - Label distribution in pointwise_df: {1.0: 12376, 0.0: 6188}\n2025-07-25 03:46:55.509754 - Prepared pointwise dataset\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"cached_split = load_cache('pointwise_split_v1')\nif cached_split is not None:\n    split = cached_split\nelse:\n    split = dataset.train_test_split(test_size=0.2, seed=SEED)\n    split = DatasetDict({'train': split['train'], 'validation': split['test']})  # Rename for clarity\n    split['train'] = split['train'].cast_column('labels', Value('float32'))\n    split['validation'] = split['validation'].cast_column('labels', Value('float32'))\n    save_cache(split, 'pointwise_split_v1')\nlog(\"Prepared pointwise dataset and split (validation from train data)\")\n\nlog(f\"Dataset columns: {dataset.column_names}\")\nlog(f\"Train split columns: {split['train'].column_names}\")\nlog(f\"Test split columns: {split['validation'].column_names}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:46:55.513394Z","iopub.execute_input":"2025-07-25T03:46:55.513955Z","iopub.status.idle":"2025-07-25T03:46:56.632832Z","shell.execute_reply.started":"2025-07-25T03:46:55.513928Z","shell.execute_reply":"2025-07-25T03:46:56.632052Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/14851 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe61f63d2c214d3b820615f548afa84a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/3713 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e570168278d04893b40252787d933617"}},"metadata":{}},{"name":"stdout","text":"2025-07-25 03:46:56.629320 - Prepared pointwise dataset and split (validation from train data)\n2025-07-25 03:46:56.629466 - Dataset columns: ['prompt', 'response', 'labels', 'length', 'sentiment', 'tfidf_sim']\n2025-07-25 03:46:56.629543 - Train split columns: ['prompt', 'response', 'labels', 'length', 'sentiment', 'tfidf_sim']\n2025-07-25 03:46:56.629630 - Test split columns: ['prompt', 'response', 'labels', 'length', 'sentiment', 'tfidf_sim']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Tokenize","metadata":{}},{"cell_type":"code","source":"model_path = '/kaggle/input/debertav3base'\n\ntokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\ntokenizer.add_special_tokens({'additional_special_tokens': ['[USER]', '[ASSISTANT]']})\nlog(\"Loaded tokenizer\")\n\ndef preprocess_function(examples):\n    inputs = ['[USER] ' + clean_text(p) + ' [ASSISTANT] ' + clean_text(r)\n              for p, r in zip(examples['prompt'], examples['response'])]\n    tokenized = tokenizer(\n        inputs,\n        truncation=True,\n        padding=True,\n        max_length=512,\n        return_tensors='pt'\n    )\n    vocab_size = len(tokenizer)\n    for i, input_ids in enumerate(tokenized['input_ids']):\n        if any(idx >= vocab_size for idx in input_ids):\n            log(f\"Warning: Invalid input_ids detected in sample {i}: {input_ids.tolist()}\")\n            raise ValueError(f\"input_ids exceed vocab size {vocab_size} in sample {i}\")\n        non_pad_tokens = [idx for idx in input_ids if idx != tokenizer.pad_token_id]\n        if len(non_pad_tokens) <= 2:\n            log(f\"Warning: Near-empty input detected in sample {i}: {input_ids.tolist()}\")\n            raise ValueError(f\"Near-empty input in sample {i}\")\n    if 'labels' in examples:\n        labels = examples['labels']\n        if not all(l in [0, 1] for l in labels):\n            log(f\"Warning: Invalid labels detected: {labels}\")\n            raise ValueError(\"Labels must be 0 or 1 only in pointwise training\")\n    input_lengths = [len([idx for idx in input_ids if idx != tokenizer.pad_token_id]) for input_ids in tokenized['input_ids']]\n    # log(f\"Input lengths stats: min={min(input_lengths)}, max={max(input_lengths)}, mean={np.mean(input_lengths):.2f}\")\n    tokenized['length'] = examples['length']\n    tokenized['sentiment'] = examples['sentiment']\n    tokenized['tfidf_sim'] = examples['tfidf_sim']\n    return tokenized\n\ncached_tokenized = load_cache('pointwise_tokenized_v1')\nif cached_tokenized is not None:\n    tokenized = cached_tokenized\nelse:\n    tokenized = split.map(\n        preprocess_function,\n        batched=True,\n        batch_size=1000,\n        remove_columns=[col for col in dataset.column_names if col not in ['input_ids', 'attention_mask', 'labels', 'length', 'sentiment', 'tfidf_sim']]\n    )\n    save_cache(tokenized, 'pointwise_tokenized_v1')\nlog(\"Tokenized pointwise data\")\nlog(f\"Tokenized train columns: {tokenized['train'].column_names}\")\nlog(f\"Tokenized test columns: {tokenized['validation'].column_names}\")\nlog(f\"Label distribution in tokenized train: {Counter(tokenized['train']['labels'])}\")\nlog(f\"Label distribution in tokenized test: {Counter(tokenized['validation']['labels'])}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:46:56.633643Z","iopub.execute_input":"2025-07-25T03:46:56.634259Z","iopub.status.idle":"2025-07-25T03:51:37.307028Z","shell.execute_reply.started":"2025-07-25T03:46:56.634239Z","shell.execute_reply":"2025-07-25T03:51:37.306381Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"2025-07-25 03:46:58.998824 - Loaded tokenizer\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14851 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"743dff7be0c14cad882083f73b033f2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3713 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"957e0aaa7e9c4df68bdd4bc21e3d45b7"}},"metadata":{}},{"name":"stdout","text":"2025-07-25 03:51:37.294844 - Tokenized pointwise data\n2025-07-25 03:51:37.294991 - Tokenized train columns: ['labels', 'length', 'sentiment', 'tfidf_sim', 'input_ids', 'token_type_ids', 'attention_mask']\n2025-07-25 03:51:37.295062 - Tokenized test columns: ['labels', 'length', 'sentiment', 'tfidf_sim', 'input_ids', 'token_type_ids', 'attention_mask']\n2025-07-25 03:51:37.302336 - Label distribution in tokenized train: Counter({1.0: 9916, 0.0: 4935})\n2025-07-25 03:51:37.304704 - Label distribution in tokenized test: Counter({1.0: 2460, 0.0: 1253})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"def stable_sigmoid(x):\n    x = np.asanyarray(x)\n    result = np.empty_like(x)\n    pos = x >= 0\n    result[pos] = 1 / (1 + np.exp(-x[pos]))\n    result[~pos] = np.exp(x[~pos]) / (1 + np.exp(x[~pos]))\n    return result\n\ndef compute_metrics(eval_pred):\n    logits = eval_pred.predictions\n    labels = eval_pred.label_ids\n    logits = np.array(logits)\n    if logits.ndim == 2 and logits.shape[1] == 1:\n        logits = logits[:, 0]\n    elif logits.ndim == 1:\n        logits = logits\n    else:\n        logits = logits.flatten()\n    labels = np.array(labels)\n    if labels.ndim > 1:\n        labels = labels.flatten()\n    min_len = min(len(labels), len(logits))\n    labels = labels[:min_len]\n    logits = logits[:min_len]\n    if len(labels) == 0:\n        return {'accuracy': 0.0, 'f1': 0.0}\n    logits = np.nan_to_num(logits, nan=0.0, posinf=709.0, neginf=-709.0)\n    logits = np.clip(logits, -20, 20)\n    probs = stable_sigmoid(logits)\n    probs = np.nan_to_num(probs, nan=0.5)\n    preds_bin = (probs > 0.5).astype(int)\n    return {\n        'accuracy': accuracy_score(labels, preds_bin),\n        'f1': f1_score(labels, preds_bin, zero_division=1)\n    }\n\nfrom transformers import DataCollatorWithPadding\nfrom torch import tensor as torch_tensor  # To avoid name conflict with built-in tensor\n\nclass CustomDataCollator(DataCollatorWithPadding):\n    def __call__(self, features):\n        batch = super().__call__(features)\n        batch[\"length\"] = torch_tensor([f[\"length\"] for f in features], dtype=torch.float32)\n        batch[\"sentiment\"] = torch_tensor([f[\"sentiment\"] for f in features], dtype=torch.float32)\n        batch[\"tfidf_sim\"] = torch_tensor([f[\"tfidf_sim\"] for f in features], dtype=torch.float32)\n        if \"labels\" in features[0]:\n            batch[\"labels\"] = torch_tensor([f[\"labels\"] for f in features], dtype=torch.float32)\n        return batch\n\n\nclass GradientLoggingCallback(TrainerCallback):\n    def on_step_end(self, args, state, control, **kwargs):\n        if state.global_step % 1000 == 0:\n            model = kwargs['model']\n            for name, param in model.named_parameters():\n                if \"classifier\" in name and param.grad is not None:\n                    print(f\"{name}: grad norm = {param.grad.norm().item()}\")\n\nclass CustomModel(nn.Module):\n    def __init__(self, model_path, tokenizer):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(model_path, local_files_only=True)\n        base = AutoModel.from_pretrained(model_path, local_files_only=True)\n        base.resize_token_embeddings(len(tokenizer))\n        self.base = base\n        self.extra_fc = nn.Linear(3, 32)\n        self.classifier = nn.Linear(self.config.hidden_size + 32, 1)\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                length=None, sentiment=None, tfidf_sim=None, labels=None, **kwargs):\n        \n        outputs = self.base(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            **kwargs\n        )\n        pooled = outputs.last_hidden_state[:, 0]\n\n        if length is None or sentiment is None or tfidf_sim is None:\n            raise ValueError(\"Extra features (length, sentiment, tfidf_sim) must not be None\")\n        extra_feats = torch.stack([\n            length.float(), sentiment.float(), tfidf_sim.float()\n        ], dim=1)\n\n        extra = self.extra_fc(extra_feats)\n        combined = torch.cat([pooled, extra], dim=1)\n        logits = self.classifier(combined)\n\n        # Ensure logits is [batch_size, 1]\n        if logits.dim() == 1:\n            logits = logits.unsqueeze(1)\n        elif logits.dim() == 2 and logits.shape[1] != 1:\n            logits = logits[:, :1]  # If multi-class by mistake, take first\n        elif logits.dim() == 0:\n            logits = logits.unsqueeze(0).unsqueeze(1)\n\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.BCEWithLogitsLoss()\n            target = labels.float().view(-1, 1) \n            loss = loss_fn(logits, target)\n        \n        return {\n            \"loss\": loss,\n            \"logits\": logits\n        }\n        \nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        inputs = self._prepare_inputs(inputs)  \n        input_ids = inputs.get(\"input_ids\")\n        attention_mask = inputs.get(\"attention_mask\")\n        token_type_ids = inputs.get(\"token_type_ids\")\n        length = inputs.get(\"length\")\n        sentiment = inputs.get(\"sentiment\")\n        tfidf_sim = inputs.get(\"tfidf_sim\")\n        labels = inputs.get(\"labels\")\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            length=length,\n            sentiment=sentiment,\n            tfidf_sim=tfidf_sim,\n            labels=labels\n        )\n        loss = outputs[\"loss\"]\n        if return_outputs:\n            return (loss, outputs)\n        else:\n            return loss\n\n    def prediction_step(self, model: nn.Module, inputs: dict, prediction_loss_only: bool, ignore_keys: list = None):\n        inputs = self._prepare_inputs(inputs)  \n        has_labels = \"labels\" in inputs\n        with torch.no_grad():\n            with self.compute_loss_context_manager():\n                outputs = model(**inputs) \n        loss = outputs[\"loss\"].mean().detach() if outputs[\"loss\"] is not None else None\n        logits = outputs[\"logits\"]\n        labels = inputs[\"labels\"] if has_labels else None\n        if prediction_loss_only:\n            return (loss, None, None)\n        return (loss, logits, labels)\n\n    def get_train_dataloader(self):\n        dataset = self.train_dataset\n        data_collator = self.data_collator\n        return DataLoader(\n            dataset,\n            batch_size=self.args.per_device_train_batch_size,\n            collate_fn=data_collator,\n            num_workers=4,\n            pin_memory=True\n        )\n\n    def get_eval_dataloader(self, eval_dataset=None):\n        dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n        data_collator = self.data_collator\n        return DataLoader(\n            dataset,\n            batch_size=self.args.per_device_eval_batch_size,\n            collate_fn=data_collator,\n            num_workers=4,\n            pin_memory=True\n        )\n\n    def __init__(self, *args, tokenizer=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.data_collator = CustomDataCollator(tokenizer=tokenizer)\n\ndef objective(trial):\n    torch.cuda.empty_cache()\n    import gc\n    gc.collect()\n    log(\"Cleared cache and ran GC in objective\")\n    learning_rate = trial.suggest_float('learning_rate', 5e-5, 5e-4, log=True)\n    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [4, 8])\n    gradient_accumulation_steps = trial.suggest_categorical('gradient_accumulation_steps', [1, 2])\n    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.1)\n    model = CustomModel(model_path, tokenizer)\n    log(\"Loaded model in objective\")\n    training_args = TrainingArguments(\n        output_dir='./results',\n        learning_rate=learning_rate,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=per_device_train_batch_size,\n        gradient_accumulation_steps=gradient_accumulation_steps,\n        num_train_epochs=3,\n        eval_strategy=\"steps\",\n        eval_steps=100,\n        save_steps=100,\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        seed=SEED,\n        report_to=\"none\",\n        logging_strategy=\"steps\",\n        logging_steps=100,\n        warmup_ratio=0.1,\n        fp16=False,\n        gradient_checkpointing=False,\n        max_grad_norm=1.0,\n        weight_decay=weight_decay,\n        adam_epsilon=1e-6\n    )\n    log(\"Set training args in objective\")\n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized['train'],\n        eval_dataset=tokenized['validation'],\n        compute_metrics=compute_metrics,\n        tokenizer=tokenizer,\n        callbacks=[GradientLoggingCallback(), EarlyStoppingCallback(early_stopping_patience=5)]\n    )\n    log(\"Initialized trainer in objective\")\n    with autocast('cuda'):\n        trainer.train()\n    log(\"Trained in objective\")\n    eval_result = trainer.evaluate()\n    log(f\"Evaluation result: {eval_result}\")\n    torch.cuda.empty_cache()\n    gc.collect()\n    return eval_result['eval_f1']\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=1)\nlog(\"Optimized hyperparameters\")\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\nmodel1 = CustomModel(model_path, tokenizer)\nlog(\"Loaded final model1\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T03:51:37.307917Z","iopub.execute_input":"2025-07-25T03:51:37.308421Z","iopub.status.idle":"2025-07-25T04:47:22.866167Z","shell.execute_reply.started":"2025-07-25T03:51:37.308394Z","shell.execute_reply":"2025-07-25T04:47:22.865320Z"}},"outputs":[{"name":"stderr","text":"[I 2025-07-25 03:51:37,329] A new study created in memory with name: no-name-55d52678-8d30-4369-85e2-b5380b8c8e0b\n","output_type":"stream"},{"name":"stdout","text":"2025-07-25 03:51:37.711112 - Cleared cache and ran GC in objective\n2025-07-25 03:51:46.373707 - Loaded model in objective\n2025-07-25 03:51:46.397739 - Set training args in objective\n2025-07-25 03:51:46.776343 - Initialized trainer in objective\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2100' max='5571' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2100/5571 53:56 < 1:29:14, 0.65 it/s, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>5.731200</td>\n      <td>5.939292</td>\n      <td>0.337463</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>5.950700</td>\n      <td>5.908082</td>\n      <td>0.337463</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>6.148900</td>\n      <td>5.849838</td>\n      <td>0.337463</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>5.675000</td>\n      <td>5.714549</td>\n      <td>0.337463</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>4.954200</td>\n      <td>4.332942</td>\n      <td>0.381632</td>\n      <td>0.234667</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>4.060900</td>\n      <td>3.649569</td>\n      <td>0.412335</td>\n      <td>0.393552</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.339600</td>\n      <td>3.318226</td>\n      <td>0.417722</td>\n      <td>0.408967</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.476100</td>\n      <td>3.068739</td>\n      <td>0.438998</td>\n      <td>0.478337</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.138100</td>\n      <td>2.895687</td>\n      <td>0.440345</td>\n      <td>0.469085</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.056900</td>\n      <td>2.761033</td>\n      <td>0.457043</td>\n      <td>0.511391</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.889400</td>\n      <td>2.690715</td>\n      <td>0.433073</td>\n      <td>0.437617</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.206700</td>\n      <td>2.623144</td>\n      <td>0.449771</td>\n      <td>0.487842</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.086900</td>\n      <td>2.562446</td>\n      <td>0.443577</td>\n      <td>0.465874</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.737700</td>\n      <td>2.488863</td>\n      <td>0.472394</td>\n      <td>0.538298</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.588900</td>\n      <td>2.442727</td>\n      <td>0.482359</td>\n      <td>0.553439</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.507500</td>\n      <td>2.392152</td>\n      <td>0.496095</td>\n      <td>0.590680</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.713300</td>\n      <td>2.360054</td>\n      <td>0.482898</td>\n      <td>0.548872</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.336800</td>\n      <td>2.333050</td>\n      <td>0.491516</td>\n      <td>0.569147</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.647400</td>\n      <td>2.320728</td>\n      <td>0.482359</td>\n      <td>0.558161</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.382300</td>\n      <td>2.312930</td>\n      <td>0.469970</td>\n      <td>0.534091</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.250100</td>\n      <td>2.312334</td>\n      <td>0.455427</td>\n      <td>0.490680</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"2025-07-25 04:45:45.872366 - Trained in objective\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='465' max='465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [465/465 01:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"2025-07-25 04:47:20.464688 - Evaluation result: {'eval_loss': 57.90950012207031, 'eval_accuracy': 0.6625370320495556, 'eval_f1': 0.797019277498785, 'eval_runtime': 94.5917, 'eval_samples_per_second': 39.253, 'eval_steps_per_second': 4.916, 'epoch': 1.1308562197092085}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-07-25 04:47:21,198] Trial 0 finished with value: 0.797019277498785 and parameters: {'learning_rate': 5.3234585773462036e-05, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 1, 'weight_decay': 0.07069410132988903}. Best is trial 0 with value: 0.797019277498785.\n","output_type":"stream"},{"name":"stdout","text":"2025-07-25 04:47:21.199927 - Optimized hyperparameters\nBest hyperparameters: {'learning_rate': 5.3234585773462036e-05, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 1, 'weight_decay': 0.07069410132988903}\n2025-07-25 04:47:22.862702 - Loaded final model1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',\n    learning_rate=best_params['learning_rate'],\n    per_device_train_batch_size=best_params['per_device_train_batch_size'],\n    #per_device_eval_batch_size=1, \n    gradient_accumulation_steps=best_params['gradient_accumulation_steps'],\n    num_train_epochs=3,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    seed=SEED,\n    report_to=\"none\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    warmup_ratio=0.1,\n    fp16=False,\n    save_total_limit=1,\n    gradient_checkpointing=False,\n    max_grad_norm=1.0,\n    weight_decay=best_params['weight_decay'],\n    adam_epsilon=1e-6\n)\nlog(\"Set final training args\")\n\ntrainer1 = CustomTrainer(\n    model=model1,\n    args=training_args,\n    train_dataset=tokenized['train'],\n    eval_dataset=tokenized['validation'],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    callbacks=[GradientLoggingCallback(), EarlyStoppingCallback(early_stopping_patience=5)]\n)\nlog(\"Initialized trainer1\")\nwith autocast('cuda'):\n    trainer1.train()\nlog(\"Trained model1\")\n\ntorch.manual_seed(SEED + 1)\ntorch.cuda.manual_seed(SEED + 1)\nmodel2 = CustomModel(model_path, tokenizer)\nlog(\"Loaded model2\")\n\ntrainer2 = CustomTrainer(\n    model=model2,\n    args=training_args,\n    train_dataset=tokenized['train'],\n    eval_dataset=tokenized['validation'],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    callbacks=[GradientLoggingCallback(), EarlyStoppingCallback(early_stopping_patience=5)]\n)\nlog(\"Initialized trainer2\")\nwith autocast('cuda'):\n    trainer2.train()\nlog(\"Trained model2\")\n\ncached_test_dataset = load_cache('test_dataset_v4')\nif cached_test_dataset is not None:\n    test_dataset = cached_test_dataset\nelse:\n    test_dataset = Dataset.from_pandas(test_df)\n    save_cache(test_dataset, 'test_dataset_v4')\nlog(\"Created test dataset\")\n\ndef add_extra_feats_pointwise(examples, response_key):\n    if response_key == 'response_a':\n        length = examples['length_a']\n        sentiment = examples['sentiment_a']\n    else:\n        length = examples['length_b']\n        sentiment = examples['sentiment_b']\n    tfidf_sim = examples['tfidf_sim']\n    return {\n        'length': length,\n        'sentiment': sentiment,\n        'tfidf_sim': tfidf_sim\n    }\n\ndef tokenize_pointwise_batch(examples, response_key):\n    inputs = ['[USER] ' + clean_text(p) + ' [ASSISTANT] ' + clean_text(r)\n              for p, r in zip(examples['prompt'], examples[response_key])]\n    tokenized = tokenizer(inputs, truncation=True, padding=True, max_length=512)\n    return tokenized\n\ncached_tokenized_test_a = load_cache('test_tokenized_a_pointwise_v1')\nif cached_tokenized_test_a is not None:\n    tokenized_test_a = cached_tokenized_test_a\nelse:\n    test_with_feats = test_dataset.map(lambda examples: add_extra_feats_pointwise(examples, 'response_a'), batched=True)\n    tokenized_test_a = test_with_feats.map(lambda examples: tokenize_pointwise_batch(examples, 'response_a'), batched=True, batch_size=1000)\n    save_cache(tokenized_test_a, 'test_tokenized_a_pointwise_v1')\n\ncached_tokenized_test_b = load_cache('test_tokenized_b_pointwise_v1')\nif cached_tokenized_test_b is not None:\n    tokenized_test_b = cached_tokenized_test_b\nelse:\n    test_with_feats = test_dataset.map(lambda examples: add_extra_feats_pointwise(examples, 'response_b'), batched=True)\n    tokenized_test_b = test_with_feats.map(lambda examples: tokenize_pointwise_batch(examples, 'response_b'), batched=True, batch_size=1000)\n    save_cache(tokenized_test_b, 'test_tokenized_b_pointwise_v1')\nlog(\"Tokenized test data for pointwise\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T04:47:22.867308Z","iopub.execute_input":"2025-07-25T04:47:22.868021Z","iopub.status.idle":"2025-07-25T07:20:26.940090Z","shell.execute_reply.started":"2025-07-25T04:47:22.867995Z","shell.execute_reply":"2025-07-25T07:20:26.939190Z"}},"outputs":[{"name":"stdout","text":"2025-07-25 04:47:22.904820 - Set final training args\n2025-07-25 04:47:23.123645 - Initialized trainer1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2600' max='5571' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2600/5571 1:05:10 < 1:14:32, 0.66 it/s, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>13.289200</td>\n      <td>13.813048</td>\n      <td>0.337463</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>13.865900</td>\n      <td>13.778483</td>\n      <td>0.337463</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>14.401200</td>\n      <td>13.695602</td>\n      <td>0.337463</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>13.336200</td>\n      <td>13.448958</td>\n      <td>0.338002</td>\n      <td>0.004858</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>12.791700</td>\n      <td>12.833281</td>\n      <td>0.344196</td>\n      <td>0.034879</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>12.439500</td>\n      <td>11.764974</td>\n      <td>0.361971</td>\n      <td>0.133821</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>10.603500</td>\n      <td>10.753362</td>\n      <td>0.389173</td>\n      <td>0.249007</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>11.097800</td>\n      <td>10.255629</td>\n      <td>0.392405</td>\n      <td>0.290120</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>10.171900</td>\n      <td>9.923219</td>\n      <td>0.394560</td>\n      <td>0.306601</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>10.217000</td>\n      <td>9.638644</td>\n      <td>0.402101</td>\n      <td>0.335727</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>9.885100</td>\n      <td>9.406054</td>\n      <td>0.405333</td>\n      <td>0.351732</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>10.748800</td>\n      <td>9.225749</td>\n      <td>0.405063</td>\n      <td>0.354282</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>10.257500</td>\n      <td>9.079633</td>\n      <td>0.406141</td>\n      <td>0.359942</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>9.484200</td>\n      <td>8.923911</td>\n      <td>0.411796</td>\n      <td>0.380952</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>9.210600</td>\n      <td>8.794502</td>\n      <td>0.411796</td>\n      <td>0.379545</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>9.110700</td>\n      <td>8.679947</td>\n      <td>0.413682</td>\n      <td>0.395110</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>9.295900</td>\n      <td>8.574372</td>\n      <td>0.412335</td>\n      <td>0.388453</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>8.145900</td>\n      <td>8.479745</td>\n      <td>0.411527</td>\n      <td>0.394233</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>9.378200</td>\n      <td>8.403085</td>\n      <td>0.416375</td>\n      <td>0.412259</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>8.330100</td>\n      <td>8.296470</td>\n      <td>0.417991</td>\n      <td>0.412612</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>8.183000</td>\n      <td>8.230876</td>\n      <td>0.423377</td>\n      <td>0.427081</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>8.879400</td>\n      <td>8.141572</td>\n      <td>0.424185</td>\n      <td>0.414567</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>8.378000</td>\n      <td>8.064998</td>\n      <td>0.412066</td>\n      <td>0.361135</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>8.612000</td>\n      <td>8.002830</td>\n      <td>0.425532</td>\n      <td>0.411586</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>7.485300</td>\n      <td>7.925646</td>\n      <td>0.414220</td>\n      <td>0.364592</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>8.360200</td>\n      <td>7.876745</td>\n      <td>0.419876</td>\n      <td>0.396639</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"2025-07-25 05:52:35.161024 - Trained model1\n2025-07-25 05:52:36.811721 - Loaded model2\n2025-07-25 05:52:37.023219 - Initialized trainer2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3500' max='5571' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3500/5571 1:27:48 < 51:58, 0.66 it/s, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>13.835000</td>\n      <td>14.379614</td>\n      <td>0.337463</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>14.442300</td>\n      <td>14.345149</td>\n      <td>0.336924</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>15.017700</td>\n      <td>14.282002</td>\n      <td>0.337463</td>\n      <td>0.001623</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>13.986800</td>\n      <td>14.165951</td>\n      <td>0.337732</td>\n      <td>0.004856</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>13.441300</td>\n      <td>13.219483</td>\n      <td>0.345273</td>\n      <td>0.044794</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>12.245000</td>\n      <td>11.434979</td>\n      <td>0.375438</td>\n      <td>0.204460</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>10.545000</td>\n      <td>10.813251</td>\n      <td>0.385403</td>\n      <td>0.263396</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>11.258300</td>\n      <td>10.424236</td>\n      <td>0.398869</td>\n      <td>0.320755</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>10.393800</td>\n      <td>10.158037</td>\n      <td>0.401831</td>\n      <td>0.334432</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>10.520800</td>\n      <td>9.931381</td>\n      <td>0.405063</td>\n      <td>0.351248</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>10.198300</td>\n      <td>9.738430</td>\n      <td>0.404794</td>\n      <td>0.353423</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>11.169100</td>\n      <td>9.583088</td>\n      <td>0.402370</td>\n      <td>0.354379</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>10.678200</td>\n      <td>9.448979</td>\n      <td>0.406141</td>\n      <td>0.370180</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>9.913100</td>\n      <td>9.339047</td>\n      <td>0.408834</td>\n      <td>0.376951</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>9.676900</td>\n      <td>9.236435</td>\n      <td>0.409103</td>\n      <td>0.375285</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>9.610100</td>\n      <td>9.150803</td>\n      <td>0.411258</td>\n      <td>0.391086</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>9.843400</td>\n      <td>9.086205</td>\n      <td>0.411258</td>\n      <td>0.394795</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>8.653300</td>\n      <td>8.998767</td>\n      <td>0.410450</td>\n      <td>0.390081</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>9.979700</td>\n      <td>8.924222</td>\n      <td>0.410988</td>\n      <td>0.394351</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>8.896900</td>\n      <td>8.866327</td>\n      <td>0.412604</td>\n      <td>0.399670</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>8.747500</td>\n      <td>8.803085</td>\n      <td>0.416375</td>\n      <td>0.409698</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>9.503800</td>\n      <td>8.730151</td>\n      <td>0.418799</td>\n      <td>0.411989</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>9.020300</td>\n      <td>8.682135</td>\n      <td>0.400754</td>\n      <td>0.320196</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>9.261500</td>\n      <td>8.620858</td>\n      <td>0.423108</td>\n      <td>0.410567</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>8.098100</td>\n      <td>8.564938</td>\n      <td>0.405333</td>\n      <td>0.344807</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>9.019800</td>\n      <td>8.512890</td>\n      <td>0.423108</td>\n      <td>0.403010</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>9.507000</td>\n      <td>8.463056</td>\n      <td>0.425801</td>\n      <td>0.416210</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>8.745000</td>\n      <td>8.430417</td>\n      <td>0.424455</td>\n      <td>0.421024</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>8.448900</td>\n      <td>8.377383</td>\n      <td>0.424993</td>\n      <td>0.412008</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>10.031400</td>\n      <td>8.349129</td>\n      <td>0.422839</td>\n      <td>0.430508</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>9.767600</td>\n      <td>8.301920</td>\n      <td>0.426879</td>\n      <td>0.413127</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>8.834200</td>\n      <td>8.282779</td>\n      <td>0.426340</td>\n      <td>0.430177</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>8.122500</td>\n      <td>8.243392</td>\n      <td>0.424993</td>\n      <td>0.423129</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>9.179100</td>\n      <td>8.203002</td>\n      <td>0.419607</td>\n      <td>0.394152</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>9.191500</td>\n      <td>8.176580</td>\n      <td>0.418529</td>\n      <td>0.390973</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"2025-07-25 07:20:26.744780 - Trained model2\n2025-07-25 07:20:26.759072 - Created test dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15116599e6564b689f4a936427ec04d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abb9a061da3b489f8f501de84b82ff72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8dfbc2cad6148d38b8b775a4e50d043"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5334a40e8d7f4e84ae532f08598f21a8"}},"metadata":{}},{"name":"stdout","text":"2025-07-25 07:20:26.936443 - Tokenized test data for pointwise\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"def flatten_logits(logits):\n    if isinstance(logits, dict):\n        logits = logits[\"logits\"]\n    if logits is None:\n        raise ValueError(\"Logits are None! Check your model's output and dataset.\")\n    logits = np.array(logits)\n    if logits.ndim == 2 and logits.shape[1] == 1:\n        logits = logits[:, 0]\n    elif logits.ndim == 0:\n        logits = np.array([logits])\n    return logits\n\nwith autocast('cuda'):\n    logits_a_model1 = flatten_logits(trainer1.predict(tokenized_test_a).predictions)\n    logits_b_model1 = flatten_logits(trainer1.predict(tokenized_test_b).predictions)\n    logits_a_model2 = flatten_logits(trainer2.predict(tokenized_test_a).predictions)\n    logits_b_model2 = flatten_logits(trainer2.predict(tokenized_test_b).predictions)\n\nprobs_a_model1 = stable_sigmoid(logits_a_model1)\nprobs_b_model1 = stable_sigmoid(logits_b_model1)\nprobs_a_model2 = stable_sigmoid(logits_a_model2)\nprobs_b_model2 = stable_sigmoid(logits_b_model2)\n\nprobs_a = (probs_a_model1 + probs_a_model2) / 2\nprobs_b = (probs_b_model1 + probs_b_model2) / 2\nlog(\"Averaged predictions\")\n\nval_logits = flatten_logits(trainer1.predict(tokenized['validation']).predictions)\nval_preds = stable_sigmoid(val_logits)\nval_labels = tokenized['validation']['labels']\n\ncalibrator = IsotonicRegression(out_of_bounds='clip')\ncalibrator.fit(val_preds, val_labels)  \n\npreds_a_calibrated = calibrator.predict(probs_a)\npreds_b_calibrated = calibrator.predict(probs_b)\nlog(\"Calibrated predictions\")\n\nthreshold = 0.05  \nwinner_a_prob = []\nwinner_b_prob = []\nwinner_tie_prob = []\nfor a, b in zip(preds_a_calibrated, preds_b_calibrated):\n    diff = a - b\n    p_tie = stable_sigmoid(-abs(diff) / threshold)  # Close to 1 if |diff| small, close to 0 if large\n    p_a_wins_given_no_tie = stable_sigmoid(diff)\n    p_b_wins_given_no_tie = 1 - p_a_wins_given_no_tie\n    winner_a_prob.append((1 - p_tie) * p_a_wins_given_no_tie)\n    winner_b_prob.append((1 - p_tie) * p_b_wins_given_no_tie)\n    winner_tie_prob.append(p_tie)\nlog(\"Computed soft probabilities\")\n\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'winner_model_a': winner_a_prob,\n    'winner_model_b': winner_b_prob,\n    'winner_tie': winner_tie_prob\n})\nsubmission_df.to_csv('submission.csv', index=False)\nlog(\"Created and saved submission.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:20:26.942708Z","iopub.execute_input":"2025-07-25T07:20:26.942921Z","iopub.status.idle":"2025-07-25T07:22:04.296118Z","shell.execute_reply.started":"2025-07-25T07:20:26.942899Z","shell.execute_reply":"2025-07-25T07:22:04.295393Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"2025-07-25 07:20:27.356498 - Averaged predictions\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"2025-07-25 07:22:04.285121 - Calibrated predictions\n2025-07-25 07:22:04.285786 - Computed soft probabilities\n2025-07-25 07:22:04.293694 - Created and saved submission.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(\"Successfully saved as CSV file\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T07:22:04.296947Z","iopub.execute_input":"2025-07-25T07:22:04.297187Z","iopub.status.idle":"2025-07-25T07:22:04.301258Z","shell.execute_reply.started":"2025-07-25T07:22:04.297171Z","shell.execute_reply":"2025-07-25T07:22:04.300553Z"}},"outputs":[{"name":"stdout","text":"Successfully saved as CSV file\n","output_type":"stream"}],"execution_count":10}]}